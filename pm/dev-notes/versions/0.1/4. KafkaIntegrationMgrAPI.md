# 4. KafkaIntegrationManager API

StreamOtter's `KafkaIntegrationManager` abstracts the complexities of Apache Kafka operations, offering a streamlined and intuitive API for producers and consumers. This manager facilitates seamless integration between backend services and Kafka topics, enabling efficient message publishing and consumption. By bridging Kafka with WebSocket management features, `KafkaIntegrationManager` ensures real-time data flow between frontend and backend systems, leveraging Kafka's scalability and reliability.

## Overview

The `KafkaIntegrationManager` provides a set of tools and functionalities for creating Kafka producers and consumers, managing message schemas, handling batch processing, overseeing offset management, and implementing robust error handling mechanisms. These features are essential for building scalable and resilient real-time data streaming applications.

## Table of Contents

- [4. KafkaIntegrationManager API](#4-kafkaintegrationmanager-api)
  - [Overview](#overview)
  - [Table of Contents](#table-of-contents)
  - [1. Kafka Producer API](#1-kafka-producer-api)
    - [Creating a Producer](#creating-a-producer)
    - [Sending Messages](#sending-messages)
  - [2. Kafka Consumer API](#2-kafka-consumer-api)
    - [Creating a Consumer](#creating-a-consumer)
    - [Starting and Stopping Consumers](#starting-and-stopping-consumers)
  - [3. Schema Registry Integration](#3-schema-registry-integration)
    - [Integrating with a Schema Registry](#integrating-with-a-schema-registry)
  - [4. Batch Processing](#4-batch-processing)
    - [Sending Messages in Batches](#sending-messages-in-batches)
    - [Consuming Messages in Batches](#consuming-messages-in-batches)
  - [5. Offset Management](#5-offset-management)
    - [Automatic Offset Management](#automatic-offset-management)
    - [Manual Offset Management](#manual-offset-management)
    - [Offset Reset Strategies](#offset-reset-strategies)
  - [6. Error Handling](#6-error-handling)
    - [Handling Producer Errors](#handling-producer-errors)
    - [Handling Consumer Errors](#handling-consumer-errors)
    - [Retry Mechanisms](#retry-mechanisms)
    - [Dead-Letter Queues](#dead-letter-queues)
  - [7. Types and Interfaces](#7-types-and-interfaces)
  - [8. Usage Examples](#8-usage-examples)
    - [Example 1: Creating and Configuring a Kafka Producer](#example-1-creating-and-configuring-a-kafka-producer)
    - [Example 2: Sending a Single Message to a Kafka Topic](#example-2-sending-a-single-message-to-a-kafka-topic)
    - [Example 3: Sending Multiple Messages in a Batch](#example-3-sending-multiple-messages-in-a-batch)
    - [Example 4: Creating and Starting a Kafka Consumer](#example-4-creating-and-starting-a-kafka-consumer)
    - [Example 5: Configuring Schema Registry Integration](#example-5-configuring-schema-registry-integration)
    - [Example 6: Handling Producer Errors and Sending to Dead-Letter Queue](#example-6-handling-producer-errors-and-sending-to-dead-letter-queue)
    - [Example 7: Handling Consumer Errors and Sending to Dead-Letter Queue](#example-7-handling-consumer-errors-and-sending-to-dead-letter-queue)
    - [Example 8: Managing Offsets with Manual Commit](#example-8-managing-offsets-with-manual-commit)
    - [Example 9: Resetting Consumer Offsets](#example-9-resetting-consumer-offsets)
    - [Example 10: Integrating Batch Processing with Offset Management](#example-10-integrating-batch-processing-with-offset-management)

---

## 1. Kafka Producer API

The Kafka Producer API allows developers to send messages to Kafka topics effortlessly. StreamOtter's `KafkaIntegrationManager` simplifies producer creation and message dispatching, abstracting the underlying Kafka client configurations.

### Creating a Producer

**`createProducer(config: KafkaProducerConfig): KafkaProducer`**

Initializes a new Kafka producer with the specified configuration. This producer can be used to send messages to Kafka topics.

- **Parameters:**
  - `config` (_KafkaProducerConfig_): Configuration settings for the Kafka producer, including broker URLs, authentication details, and serialization settings.

```typescript
interface KafkaProducerConfig {
  brokers: string[]; // List of Kafka broker URLs
  clientId: string; // Identifier for the producer client
  ssl?: SSLConfig; // Optional SSL configuration
  sasl?: SASLConfig; // Optional SASL authentication configuration
  retry?: RetryConfig; // Optional retry settings
  // Additional producer-specific configurations
}

interface SSLConfig {
  rejectUnauthorized: boolean;
  // Other SSL-related settings
}

interface SASLConfig {
  mechanism: "plain" | "scram-sha-256" | "scram-sha-512";
  username: string;
  password: string;
}

interface RetryConfig {
  retries: number; // Number of retry attempts
  retryTimeout: number; // Timeout between retries in milliseconds
}
```

### Sending Messages

**`sendMessage(producer: KafkaProducer, topic: string, messages: ProducerMessage | ProducerMessage[], options?: SendOptions): Promise<void>`**

Sends a single message or an array of messages to a specified Kafka topic using the provided producer.

- **Parameters:**
  - `producer` (_KafkaProducer_): The Kafka producer instance used to send messages.
  - `topic` (_string_): The name of the Kafka topic to send messages to.
  - `messages` (_ProducerMessage | ProducerMessage[]_): The message or array of messages to be sent.
  - `options` (_SendOptions_, optional): Additional options for message dispatching, such as specifying the partition or key.

```typescript
interface ProducerMessage {
  key?: string | Buffer; // Optional key for partitioning
  value: string | Buffer | object; // Message payload
  headers?: Record<string, string>; // Optional headers
}

interface SendOptions {
  partition?: number; // Specific partition to send the message to
  timestamp?: number; // Timestamp for the message
}
```

---

## 2. Kafka Consumer API

The Kafka Consumer API enables applications to subscribe to Kafka topics and process incoming messages. StreamOtter's `KafkaIntegrationManager` offers a simplified approach to creating and managing consumers, ensuring efficient message handling and processing.

### Creating a Consumer

**`createConsumer(config: KafkaConsumerConfig, topics: string | string[], messageHandler: (message: ConsumerMessage) => void): KafkaConsumer`**

Initializes a new Kafka consumer with the specified configuration and subscribes it to the provided topics. The `messageHandler` callback is invoked for each message received.

- **Parameters:**
  - `config` (_KafkaConsumerConfig_): Configuration settings for the Kafka consumer, including broker URLs, group ID, authentication details, and deserialization settings.
  - `topics` (_string | string[]_): The Kafka topic or topics to subscribe to.
  - `messageHandler` (_function_): A callback function that processes each incoming message.

```typescript
interface KafkaConsumerConfig {
  brokers: string[]; // List of Kafka broker URLs
  groupId: string; // Consumer group identifier
  clientId: string; // Identifier for the consumer client
  ssl?: SSLConfig; // Optional SSL configuration
  sasl?: SASLConfig; // Optional SASL authentication configuration
  // Additional consumer-specific configurations
}

interface ConsumerMessage {
  topic: string; // Name of the topic the message was received from
  partition: number; // Partition number
  offset: number; // Message offset
  key?: string | Buffer; // Optional key
  value: string | Buffer | object; // Message payload
  headers?: Record<string, string>; // Optional headers
}
```

### Starting and Stopping Consumers

**`startConsumer(consumer: KafkaConsumer): Promise<void>`**

Starts the message consumption process for the specified consumer. This method is typically called immediately after creating a consumer.

- **Parameters:**
  - `consumer` (_KafkaConsumer_): The Kafka consumer instance to start.

**`stopConsumer(consumer: KafkaConsumer): Promise<void>`**

Stops the message consumption process for the specified consumer. This is useful for gracefully shutting down consumers or when altering message handlers.

- **Parameters:**
  - `consumer` (_KafkaConsumer_): The Kafka consumer instance to stop.

---

## 3. Schema Registry Integration

Ensuring consistent and compatible message schemas is vital for reliable data processing. StreamOtter's `KafkaIntegrationManager` integrates with schema registries to enforce message schemas, enabling schema evolution and compatibility checks.

### Integrating with a Schema Registry

**`setSchemaRegistry(config: SchemaRegistryConfig): void`**

Configures the connection to a schema registry service. This allows producers and consumers to register, retrieve, and validate message schemas.

- **Parameters:**
  - `config` (_SchemaRegistryConfig_): Configuration settings for the schema registry, including the registry URL and authentication details.

```typescript
interface SchemaRegistryConfig {
  url: string; // URL of the schema registry service
  basicAuth?: {
    username: string;
    password: string;
  };
  // Additional schema registry-specific configurations
}
```

**`registerSchema(subject: string, schema: object): Promise<number>`**

Registers a new schema under the specified subject in the schema registry. Returns the schema ID assigned by the registry.

- **Parameters:**
  - `subject` (_string_): The subject under which the schema is registered.
  - `schema` (_object_): The schema definition.

**`getSchema(schemaId: number): Promise<object>`**

Retrieves the schema associated with the given schema ID from the schema registry.

- **Parameters:**
  - `schemaId` (_number_): The unique identifier of the schema.

---

## 4. Batch Processing

Processing messages in batches can significantly enhance performance and throughput, especially in high-volume data streaming scenarios. StreamOtter's `KafkaIntegrationManager` supports batch processing for both producers and consumers, allowing efficient handling of multiple messages simultaneously.

### Sending Messages in Batches

**`sendMessages(producer: KafkaProducer, topic: string, messages: ProducerMessage[], options?: SendOptions): Promise<void>`**

Sends an array of messages to a specified Kafka topic in a single batch using the provided producer.

- **Parameters:**
  - `producer` (_KafkaProducer_): The Kafka producer instance used to send messages.
  - `topic` (_string_): The name of the Kafka topic to send messages to.
  - `messages` (_ProducerMessage[]_): An array of messages to be sent.
  - `options` (_SendOptions_, optional): Additional options for message dispatching.

### Consuming Messages in Batches

**`consumeBatch(consumer: KafkaConsumer, batchSize: number, batchHandler: (messages: ConsumerMessage[]) => void): void`**

Processes incoming messages in batches. Accumulates messages up to the specified `batchSize` before invoking the `batchHandler` callback.

- **Parameters:**
  - `consumer` (_KafkaConsumer_): The Kafka consumer instance from which to consume messages.
  - `batchSize` (_number_): The number of messages to accumulate before processing.
  - `batchHandler` (_function_): A callback function that processes an array of messages.

---

## 5. Offset Management

Managing message offsets is crucial for ensuring that consumers process messages exactly once and can recover gracefully from failures. StreamOtter's `KafkaIntegrationManager` provides fine-grained control over offset commits, allowing manual and automatic offset management strategies.

### Automatic Offset Management

By default, consumers can be configured to commit offsets automatically at specified intervals. This simplifies offset tracking but may not be suitable for all use cases.

**`setAutoCommit(consumer: KafkaConsumer, enable: boolean, interval?: number): void`**

Enables or disables automatic offset commits for a consumer. If enabled, offsets are committed at the specified interval.

- **Parameters:**
  - `consumer` (_KafkaConsumer_): The Kafka consumer instance.
  - `enable` (_boolean_): Whether to enable automatic offset commits.
  - `interval` (_number_, optional): The interval in milliseconds between automatic commits.

### Manual Offset Management

For scenarios requiring precise control over when offsets are committed (e.g., after successful processing), manual offset management is essential.

**`commitOffset(consumer: KafkaConsumer, message: ConsumerMessage): Promise<void>`**

Manually commits the offset of a specific message, ensuring that the consumer acknowledges processing of that message.

- **Parameters:**
  - `consumer` (_KafkaConsumer_): The Kafka consumer instance.
  - `message` (_ConsumerMessage_): The message whose offset is to be committed.

**`commitBatchOffsets(consumer: KafkaConsumer, messages: ConsumerMessage[]): Promise<void>`**

Manually commits the offsets of an array of messages in a single batch operation.

- **Parameters:**
  - `consumer` (_KafkaConsumer_): The Kafka consumer instance.
  - `messages` (_ConsumerMessage[]_): An array of messages whose offsets are to be committed.

### Offset Reset Strategies

In cases where a consumer needs to reset its offset (e.g., to reprocess messages), StreamOtter provides mechanisms to specify offset reset behaviors.

**`resetOffsets(consumer: KafkaConsumer, strategy: OffsetResetStrategy): Promise<void>`**

Resets the consumer's offsets based on the specified strategy.

- **Parameters:**
  - `consumer` (_KafkaConsumer_): The Kafka consumer instance.
  - `strategy` (_OffsetResetStrategy_): The strategy defining how to reset offsets.

```typescript
type OffsetResetStrategy = "earliest" | "latest" | "none";
```

---

## 6. Error Handling

Robust error handling is essential for maintaining the reliability and stability of Kafka integrations. StreamOtter's `KafkaIntegrationManager` incorporates comprehensive error management strategies, including retries, dead-letter queues, and alerting mechanisms.

### Handling Producer Errors

**`onProducerError(producer: KafkaProducer, handler: (error: Error, message?: ProducerMessage) => void): void`**

Registers an error handler for producer-related errors. This allows developers to define custom responses to production failures.

- **Parameters:**
  - `producer` (_KafkaProducer_): The Kafka producer instance.
  - `handler` (_function_): A callback function that handles the error and optionally the message that failed to send.

### Handling Consumer Errors

**`onConsumerError(consumer: KafkaConsumer, handler: (error: Error, message?: ConsumerMessage) => void): void`**

Registers an error handler for consumer-related errors. This facilitates custom error responses, such as logging or alerting.

- **Parameters:**
  - `consumer` (_KafkaConsumer_): The Kafka consumer instance.
  - `handler` (_function_): A callback function that handles the error and optionally the message that caused the error.

### Retry Mechanisms

StreamOtter allows configuring retry policies for both producers and consumers to handle transient errors gracefully.

**`setRetryPolicy(config: RetryPolicyConfig): void`**

Configures the global retry policy for Kafka operations, including the number of retry attempts and backoff strategies.

- **Parameters:**
  - `config` (_RetryPolicyConfig_): Configuration settings for retry behavior.

```typescript
interface RetryPolicyConfig {
  maxAttempts: number; // Maximum number of retry attempts
  backoffStrategy: "exponential" | "linear" | "fixed"; // Type of backoff strategy
  backoffDelay: number; // Initial delay between retries in milliseconds
  factor?: number; // Multiplicative factor for exponential backoff
  jitter?: boolean; // Whether to apply jitter to delay times
}
```

### Dead-Letter Queues

For messages that consistently fail to process, StreamOtter supports dead-letter queues (DLQs) to isolate problematic messages and prevent them from blocking the processing pipeline.

**`setDeadLetterQueue(config: DeadLetterQueueConfig): void`**

Configures the dead-letter queue settings, including the destination topic for failed messages and retry policies.

- **Parameters:**
  - `config` (_DeadLetterQueueConfig_): Configuration settings for the dead-letter queue.

```typescript
interface DeadLetterQueueConfig {
  topic: string; // Kafka topic designated as the dead-letter queue
  retryPolicy?: RetryPolicyConfig; // Optional retry policy for dead-letter processing
}
```

**`sendToDeadLetterQueue(message: ProducerMessage, reason: string): Promise<void>`**

Sends a failed message to the configured dead-letter queue with an accompanying reason for the failure.

- **Parameters:**
  - `message` (_ProducerMessage_): The message to be sent to the dead-letter queue.
  - `reason` (_string_): A description of why the message was moved to the dead-letter queue.

---

## 7. Types and Interfaces

To ensure clarity and consistency across the `KafkaIntegrationManager API`, the following types and interfaces are defined:

```typescript
// Producer-related Interfaces
interface KafkaProducerConfig {
  brokers: string[];
  clientId: string;
  ssl?: SSLConfig;
  sasl?: SASLConfig;
  retry?: RetryConfig;
  // Additional configurations
}

interface ProducerMessage {
  key?: string | Buffer;
  value: string | Buffer | object;
  headers?: Record<string, string>;
}

interface SendOptions {
  partition?: number;
  timestamp?: number;
}

// Consumer-related Interfaces
interface KafkaConsumerConfig {
  brokers: string[];
  groupId: string;
  clientId: string;
  ssl?: SSLConfig;
  sasl?: SASLConfig;
  // Additional configurations
}

interface ConsumerMessage {
  topic: string;
  partition: number;
  offset: number;
  key?: string | Buffer;
  value: string | Buffer | object;
  headers?: Record<string, string>;
}

// Security-related Interfaces
interface SSLConfig {
  rejectUnauthorized: boolean;
  // Other SSL settings
}

interface SASLConfig {
  mechanism: "plain" | "scram-sha-256" | "scram-sha-512";
  username: string;
  password: string;
}

// Retry-related Interfaces
interface RetryConfig {
  retries: number;
  retryTimeout: number;
}

interface RetryPolicyConfig {
  maxAttempts: number;
  backoffStrategy: "exponential" | "linear" | "fixed";
  backoffDelay: number;
  factor?: number;
  jitter?: boolean;
}

// Dead-Letter Queue Interfaces
interface DeadLetterQueueConfig {
  topic: string;
  retryPolicy?: RetryPolicyConfig;
}

// Schema Registry Interfaces
interface SchemaRegistryConfig {
  url: string;
  basicAuth?: {
    username: string;
    password: string;
  };
  // Additional configurations
}

// Offset Management Interfaces
type OffsetResetStrategy = "earliest" | "latest" | "none";
```

---

## 8. Usage Examples

To demonstrate the practical application of the `KafkaIntegrationManager API`, the following usage examples cover common scenarios developers may encounter. Each example includes a brief explanation followed by the corresponding TypeScript code snippet.

---

### Example 1: Creating and Configuring a Kafka Producer

**Purpose:**  
Initialize a Kafka producer with specific configurations, including broker URLs, client ID, SSL, SASL authentication, and retry settings. This setup prepares the producer to send messages securely and reliably to Kafka topics.

**Code Snippet:**

```typescript
import { KafkaIntegrationManager, KafkaProducerConfig } from "your-library";

// Define Kafka producer configuration
const producerConfig: KafkaProducerConfig = {
  brokers: ["broker1.example.com:9092", "broker2.example.com:9092"],
  clientId: "streamotter-producer",
  ssl: {
    rejectUnauthorized: true,
    // Additional SSL settings if necessary
  },
  sasl: {
    mechanism: "scram-sha-512",
    username: "producerUser",
    password: "producerPass",
  },
  retry: {
    retries: 5,
    retryTimeout: 3000, // 3 seconds
  },
};

// Create Kafka producer
const producer = KafkaIntegrationManager.createProducer(producerConfig);

console.log("Kafka producer created successfully.");
```

---

### Example 2: Sending a Single Message to a Kafka Topic

**Purpose:**  
Demonstrate how to send an individual message to a specified Kafka topic using the previously created producer. This example includes optional parameters like partition and timestamp to control message placement and metadata.

**Code Snippet:**

```typescript
import {
  KafkaIntegrationManager,
  ProducerMessage,
  SendOptions,
} from "your-library";

// Define the message to be sent
const message: ProducerMessage = {
  key: "user123",
  value: { action: "login", timestamp: Date.now() },
  headers: { source: "webapp" },
};

// Define send options
const sendOptions: SendOptions = {
  partition: 0, // Optional: Specify the partition number
  timestamp: Date.now(), // Optional: Assign a timestamp to the message
};

// Send the message to the "user-actions" topic
KafkaIntegrationManager.sendMessage(
  producer,
  "user-actions",
  message,
  sendOptions
)
  .then(() => console.log("Message sent successfully"))
  .catch((error) => console.error("Failed to send message:", error));
```

---

### Example 3: Sending Multiple Messages in a Batch

**Purpose:**  
Illustrate how to send an array of messages to a Kafka topic in a single batch operation. Batch processing can improve performance and throughput by reducing the number of network calls.

**Code Snippet:**

```typescript
import {
  KafkaIntegrationManager,
  ProducerMessage,
  SendOptions,
} from "your-library";

// Define multiple messages to be sent
const batchMessages: ProducerMessage[] = [
  {
    key: "user124",
    value: { action: "logout", timestamp: Date.now() },
    headers: { source: "webapp" },
  },
  {
    key: "user125",
    value: { action: "purchase", timestamp: Date.now(), item: "book" },
    headers: { source: "mobile" },
  },
  // Add more messages as needed
];

// Define send options
const sendOptions: SendOptions = {
  partition: 1, // Optional: Specify the partition number
  timestamp: Date.now(), // Optional: Assign a timestamp to the messages
};

// Send the batch of messages to the "user-actions" topic
KafkaIntegrationManager.sendMessages(
  producer,
  "user-actions",
  batchMessages,
  sendOptions
)
  .then(() => console.log("Batch messages sent successfully"))
  .catch((error) => console.error("Failed to send batch messages:", error));
```

---

### Example 4: Creating and Starting a Kafka Consumer

**Purpose:**  
Set up a Kafka consumer that subscribes to specific topics and processes incoming messages using a designated message handler. This example includes configurations for broker URLs, group ID, client ID, SSL, and SASL authentication.

**Code Snippet:**

```typescript
import {
  KafkaIntegrationManager,
  KafkaConsumerConfig,
  ConsumerMessage,
} from "your-library";

// Define Kafka consumer configuration
const consumerConfig: KafkaConsumerConfig = {
  brokers: ["broker1.example.com:9092", "broker2.example.com:9092"],
  groupId: "streamotter-consumer-group",
  clientId: "streamotter-consumer",
  ssl: {
    rejectUnauthorized: true,
    // Additional SSL settings if necessary
  },
  sasl: {
    mechanism: "scram-sha-512",
    username: "consumerUser",
    password: "consumerPass",
  },
};

// Define the message handler function
const messageHandler = (message: ConsumerMessage) => {
  console.log(`Received message from ${message.topic}:`, message.value);
  // Implement message processing logic here
};

// Create Kafka consumer subscribed to the "user-actions" topic
const consumer = KafkaIntegrationManager.createConsumer(
  consumerConfig,
  ["user-actions"],
  messageHandler
);

// Start the consumer to begin processing messages
KafkaIntegrationManager.startConsumer(consumer)
  .then(() => console.log("Kafka consumer started successfully"))
  .catch((error) => console.error("Failed to start Kafka consumer:", error));
```

---

### Example 5: Configuring Schema Registry Integration

**Purpose:**  
Integrate with a schema registry to enforce message schemas, ensuring that all messages adhere to predefined structures. This example includes setting up the schema registry and registering a new schema for a specific subject.

**Code Snippet:**

```typescript
import { KafkaIntegrationManager, SchemaRegistryConfig } from "your-library";

// Define schema registry configuration
const schemaRegistryConfig: SchemaRegistryConfig = {
  url: "https://schema-registry.example.com",
  basicAuth: {
    username: "schemaUser",
    password: "schemaPass",
  },
};

// Configure the schema registry within the KafkaIntegrationManager
KafkaIntegrationManager.setSchemaRegistry(schemaRegistryConfig);

// Define a schema for the "user-actions" subject
const userActionsSchema = {
  type: "record",
  name: "UserAction",
  fields: [
    { name: "userId", type: "string" },
    { name: "action", type: "string" },
    { name: "timestamp", type: "long" },
  ],
};

// Register the schema and get its ID
KafkaIntegrationManager.registerSchema("user-actions", userActionsSchema)
  .then((schemaId) => {
    console.log(`Schema registered with ID: ${schemaId}`);
  })
  .catch((error) => {
    console.error("Failed to register schema:", error);
  });
```

---

### Example 6: Handling Producer Errors and Sending to Dead-Letter Queue

**Purpose:**  
Manage errors encountered by the Kafka producer by sending problematic messages to a dead-letter queue (DLQ). This ensures that failed messages are isolated and can be reviewed or reprocessed without disrupting the main message flow.

**Code Snippet:**

```typescript
import {
  KafkaIntegrationManager,
  DeadLetterQueueConfig,
  ProducerMessage,
} from "your-library";

// Configure Dead-Letter Queue (DLQ)
const dlqConfig: DeadLetterQueueConfig = {
  topic: "dead-letter-queue",
  retryPolicy: {
    maxAttempts: 3,
    backoffStrategy: "fixed",
    backoffDelay: 2000, // 2 seconds
  },
};

KafkaIntegrationManager.setDeadLetterQueue(dlqConfig);

// Handle Producer Errors
KafkaIntegrationManager.onProducerError(producer, (error, message) => {
  console.error("Producer encountered an error:", error.message);
  if (message) {
    KafkaIntegrationManager.sendToDeadLetterQueue(message, error.message)
      .then(() => console.log("Message sent to Dead-Letter Queue"))
      .catch((dlqError) =>
        console.error("Failed to send message to Dead-Letter Queue:", dlqError)
      );
  }
});

// Example of sending a message that might fail
const faultyMessage: ProducerMessage = {
  key: "userFaulty",
  value: { action: "invalid-action", timestamp: Date.now() },
  headers: { source: "webapp" },
};

KafkaIntegrationManager.sendMessage(producer, "user-actions", faultyMessage)
  .then(() => console.log("Faulty message sent successfully"))
  .catch((error) => {
    console.error("Error sending faulty message:", error);
    // The onProducerError handler will handle sending to DLQ
  });
```

---

### Example 7: Handling Consumer Errors and Sending to Dead-Letter Queue

**Purpose:**  
Manage errors encountered by the Kafka consumer by sending problematic messages to a dead-letter queue (DLQ). This ensures that failed messages are isolated and can be reviewed or reprocessed without disrupting the main message flow.

**Code Snippet:**

```typescript
import {
  KafkaIntegrationManager,
  DeadLetterQueueConfig,
  ConsumerMessage,
} from "your-library";

// Configure Dead-Letter Queue (DLQ) if not already set
const dlqConfig: DeadLetterQueueConfig = {
  topic: "dead-letter-queue",
  retryPolicy: {
    maxAttempts: 3,
    backoffStrategy: "fixed",
    backoffDelay: 2000, // 2 seconds
  },
};

KafkaIntegrationManager.setDeadLetterQueue(dlqConfig);

// Handle Consumer Errors
KafkaIntegrationManager.onConsumerError(consumer, (error, message) => {
  console.error("Consumer encountered an error:", error.message);
  if (message) {
    const dlqMessage: ProducerMessage = {
      key: message.key,
      value: message.value,
      headers: { originalOffset: message.offset.toString() },
    };
    KafkaIntegrationManager.sendToDeadLetterQueue(dlqMessage, error.message)
      .then(() => console.log("Failed message sent to Dead-Letter Queue"))
      .catch((dlqError) =>
        console.error("Failed to send message to Dead-Letter Queue:", dlqError)
      );
  }
});

// Example of processing messages with potential errors in the handler
const messageHandler = (message: ConsumerMessage) => {
  try {
    if (message.value.action === "invalid-action") {
      throw new Error("Invalid action detected");
    }
    console.log(`Processing message: ${JSON.stringify(message.value)}`);
    // Implement additional processing logic here
  } catch (error) {
    // Trigger the consumer error handler
    KafkaIntegrationManager.onConsumerError(consumer, (err, failedMessage) => {
      console.error("Error processing message:", err.message);
      // DLQ handling is already managed by the onConsumerError handler
    })(error as Error, message);
  }
};

// Re-create consumer with the updated message handler
const consumer = KafkaIntegrationManager.createConsumer(
  consumerConfig,
  ["user-actions"],
  messageHandler
);

// Start the consumer
KafkaIntegrationManager.startConsumer(consumer)
  .then(() => console.log("Kafka consumer started successfully"))
  .catch((error) => console.error("Failed to start Kafka consumer:", error));
```

---

### Example 8: Managing Offsets with Manual Commit

**Purpose:**  
Demonstrate how to manually commit message offsets after successful processing. This approach ensures that messages are only marked as consumed once they have been reliably processed, providing greater control and reliability.

**Code Snippet:**

```typescript
import {
  KafkaIntegrationManager,
  KafkaConsumerConfig,
  ConsumerMessage,
} from "your-library";

// Define Kafka consumer configuration
const consumerConfig: KafkaConsumerConfig = {
  brokers: ["broker1.example.com:9092", "broker2.example.com:9092"],
  groupId: "streamotter-consumer-group",
  clientId: "streamotter-consumer",
  ssl: {
    rejectUnauthorized: true,
    // Additional SSL settings if necessary
  },
  sasl: {
    mechanism: "scram-sha-512",
    username: "consumerUser",
    password: "consumerPass",
  },
};

// Define the message handler with manual offset commit
const messageHandler = async (message: ConsumerMessage) => {
  try {
    console.log(`Processing message from ${message.topic}:`, message.value);
    // Implement message processing logic here

    // After successful processing, commit the offset
    await KafkaIntegrationManager.commitOffset(consumer, message);
    console.log(`Offset ${message.offset} committed successfully.`);
  } catch (error) {
    console.error("Error processing message:", (error as Error).message);
    // Optionally, send to Dead-Letter Queue or handle the error accordingly
  }
};

// Create Kafka consumer subscribed to the "user-actions" topic
const consumer = KafkaIntegrationManager.createConsumer(
  consumerConfig,
  ["user-actions"],
  messageHandler
);

// Start the consumer to begin processing messages
KafkaIntegrationManager.startConsumer(consumer)
  .then(() => console.log("Kafka consumer started successfully"))
  .catch((error) => console.error("Failed to start Kafka consumer:", error));
```

---

### Example 9: Resetting Consumer Offsets

**Purpose:**  
Illustrate how to reset consumer offsets based on a specified strategy. This is useful for reprocessing messages or handling specific recovery scenarios.

**Code Snippet:**

```typescript
import { KafkaIntegrationManager, OffsetResetStrategy } from "your-library";

// Define the offset reset strategy
const resetStrategy: OffsetResetStrategy = "earliest"; // Options: 'earliest', 'latest', 'none'

// Reset consumer offsets
KafkaIntegrationManager.resetOffsets(consumer, resetStrategy)
  .then(() => console.log(`Offsets reset to ${resetStrategy} successfully.`))
  .catch((error) => console.error("Failed to reset offsets:", error));
```

---

### Example 10: Integrating Batch Processing with Offset Management

**Purpose:**  
Combine batch processing with manual offset management to efficiently handle multiple messages while ensuring reliable consumption tracking.

**Code Snippet:**

```typescript
import { KafkaIntegrationManager, ConsumerMessage } from "your-library";

// Define batch size
const BATCH_SIZE = 10;

// Define batch handler function
const batchHandler = async (messages: ConsumerMessage[]) => {
  try {
    console.log(`Processing batch of ${messages.length} messages.`);
    for (const message of messages) {
      console.log(`Processing message from ${message.topic}:`, message.value);
      // Implement individual message processing logic here
    }

    // Commit offsets for all processed messages
    await KafkaIntegrationManager.commitBatchOffsets(consumer, messages);
    console.log("Batch offsets committed successfully.");
  } catch (error) {
    console.error("Error processing batch:", (error as Error).message);
    // Optionally, handle errors or send to Dead-Letter Queue
  }
};

// Start batch consumption
KafkaIntegrationManager.consumeBatch(consumer, BATCH_SIZE, batchHandler);
```
